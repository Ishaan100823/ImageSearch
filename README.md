# ğŸ” Multi-Model Image Search System

A sophisticated AI-powered visual similarity search platform with specialized model support. Built with CLIP and FAISS, featuring modular architecture, multi-model training capabilities, and modern web interfaces.

## âœ¨ Key Features

- **ğŸ¯ Multi-Model Architecture**: Train and deploy specialized models for different product categories
- **ğŸ§  AI-Powered Search**: OpenAI CLIP for deep visual understanding and feature extraction
- **âš¡ Lightning Fast**: FAISS indexing delivers sub-second search results (< 200ms)
- **ğŸ”„ Smart Resume**: Comprehensive checkpoint system prevents work duplication
- **ğŸ—ï¸ Modular Design**: Clean separation of concerns with organized codebase
- **ğŸ“Š Advanced Pipeline**: 4-phase processing with intelligent progress tracking
- **ğŸ–¥ï¸ Modern UI**: Enhanced Streamlit interface with model selection
- **ğŸš€ Production Ready**: FastAPI backend with multiple endpoints
- **ğŸ“ˆ Horizontally Scalable**: Support for unlimited specialized models

## ğŸ—ï¸ Architecture Overview

```
ğŸ“ ImageSearch/
â”œâ”€â”€ ğŸ“‚ core/                        # Configuration & Model Management
â”‚   â”œâ”€â”€ config.py                   # System configuration
â”‚   â”œâ”€â”€ model_config.py             # Multi-model definitions
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ pipeline/                    # Data Processing & ML Pipeline
â”‚   â”œâ”€â”€ process_csv_data.py         # Data validation & image downloading
â”‚   â”œâ”€â”€ build_engine.py             # CLIP extraction & FAISS indexing
â”‚   â”œâ”€â”€ main.py                     # Pipeline orchestrator
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ api/                         # Backend Services
â”‚   â”œâ”€â”€ main_api.py                 # Original single-model API
â”‚   â”œâ”€â”€ enhanced_api.py             # Multi-model API with routing
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ training/                    # Model Training
â”‚   â”œâ”€â”€ train_model.py              # Universal training script
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ ui/                          # Frontend Interfaces
â”‚   â”œâ”€â”€ app_streamlit.py            # Enhanced multi-model UI
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ utils/                       # Shared Utilities
â”‚   â”œâ”€â”€ logging_utils.py            # Advanced logging & progress tracking
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ ğŸ“‚ data/                        # Organized by Model
â”‚   â”œâ”€â”€ general/                    # General product model data
â”‚   â”‚   â”œâ”€â”€ products.csv
â”‚   â”‚   â”œâ”€â”€ image_cache/
â”‚   â”‚   â””â”€â”€ processed_products.json
â”‚   â””â”€â”€ shirts/                     # Specialized shirts model data
â”‚       â”œâ”€â”€ products.csv
â”‚       â””â”€â”€ image_cache/
â”œâ”€â”€ ğŸ“‚ models/                      # Trained Models
â”‚   â”œâ”€â”€ general/                    # General model artifacts
â”‚   â”‚   â”œâ”€â”€ index.faiss
â”‚   â”‚   â”œâ”€â”€ metadata.pkl
â”‚   â”‚   â””â”€â”€ embeddings.npy
â”‚   â””â”€â”€ shirts/                     # Shirts model artifacts
â”œâ”€â”€ ğŸ“‚ checkpoints/                 # Resume Points
â”œâ”€â”€ ğŸ“‚ logs/                        # System Logs
â”œâ”€â”€ run_pipeline.py                 # ğŸš€ Pipeline launcher
â”œâ”€â”€ run_training.py                 # ğŸ¯ Training launcher
â”œâ”€â”€ run_api.py                      # ğŸŒ API launcher
â”œâ”€â”€ run_streamlit.py                # ğŸ–¥ï¸ UI launcher
â”œâ”€â”€ products.csv                    # Original data file
â””â”€â”€ requirements.txt                # Dependencies
```

## ğŸ¯ Multi-Model System

### Available Models

- **ğŸª General Product Search**: Multi-category product search (your current 175 products)
- **ğŸ‘” Specialized Shirts Search**: Dedicated shirts model (ready for 3-4k images)
- **â• Extensible**: Easy to add new specialized models

### API Endpoints

```bash
# Model Information
GET  /models          # List all available models
GET  /health          # System status & model info

# Specialized Search Endpoints
POST /general/search  # Search general model
POST /shirts/search   # Search shirts model
POST /all/search      # Search across all models

# Legacy Support
POST /search_products/ # Backward compatibility (uses general model)
```

## âš¡ Quick Start

### 1. Installation

```bash
# Clone and setup environment
git clone <repository>
cd ImageSearch
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Validate installation
python test_setup.py
```

### 2. Train Your First Model

```bash
# Your current general model (already trained)
python run_training.py general

# Train a new shirts model (place CSV in data/shirts/products.csv)
python run_training.py shirts --force

# List available models
python run_training.py --list
```

### 3. Start the System

```bash
# Start multi-model API backend
python run_api.py

# Start enhanced Streamlit frontend
python run_streamlit.py
```

**Access Points:**
- **ğŸ–¥ï¸ Enhanced UI**: http://localhost:8501 (Multi-model interface)
- **ğŸ“– API Docs**: http://localhost:8000/docs (Interactive documentation)
- **ğŸ” Models Info**: http://localhost:8000/models (Available models)

## ğŸ”§ Configuration & Customization

### Adding New Models

1. **Define Model** in `core/model_config.py`:
```python
MODELS = {
    "your_model": {
        "name": "Your Model Name",
        "description": "Model description",
        "data_dir": str(DATA_DIR / "your_model"),
        "models_dir": str(MODELS_DIR / "your_model"),
        "csv_file": "products.csv",
        "categories": ["Category1", "Category2"],
        "api_path": "/your_model",
        "port_offset": 2
    }
}
```

2. **Prepare Data**:
```bash
mkdir data/your_model
# Place your products.csv in data/your_model/
```

3. **Train Model**:
```bash
python run_training.py your_model --force
```

### Data Format

Your CSV files should follow this structure:

```csv
shopify_product_id,title,images,preview_image,category
123456789,"Product Title","[""url1.jpg"", ""url2.jpg""]","preview.jpg","Category"
```

**Required Columns:**
- `shopify_product_id`: Unique identifier
- `title`: Product name/description
- `images`: JSON array of image URLs (up to 10 images)
- `preview_image`: Primary display image

**Optional Columns:**
- `category`: Product category for filtering

## ğŸš€ Usage Examples

### Python API

```python
import requests

# Search general model
response = requests.post(
    "http://localhost:8000/general/search",
    files={"file": open("query_image.jpg", "rb")},
    params={"top_k": 5}
)
results = response.json()

# Search all models
response = requests.post(
    "http://localhost:8000/all/search",
    files={"file": open("query_image.jpg", "rb")},
    params={"top_k": 3}
)
all_results = response.json()
```

### Command Line

```bash
# Train models
python run_training.py general --force
python run_training.py shirts --force

# Run pipeline only (no API)
python run_pipeline.py --mode data    # Data processing only
python run_pipeline.py --mode ml      # ML pipeline only
python run_pipeline.py --mode all     # Complete pipeline
```

## ğŸ“Š Pipeline Phases

### Phase 1: Data Validation
- Validates CSV structure and required columns
- Handles malformed JSON arrays in image URLs
- Reports data quality statistics
- **Checkpoint**: `checkpoints/data_validation.json`

### Phase 2: Image Download & Caching
- Concurrent download with 8 workers
- Image validation (format, size, integrity)
- Organized cache by product ID
- **Checkpoint**: `checkpoints/image_download.json`

### Phase 3: CLIP Feature Extraction
- OpenAI CLIP model: `openai/clip-vit-base-patch32`
- Batch processing for efficiency
- Multi-image aggregation (averages multiple views)
- L2 normalization for cosine similarity
- **Checkpoint**: `checkpoints/feature_extraction.json`

### Phase 4: FAISS Index Building
- Creates searchable FAISS index
- Supports multiple index types (Flat, IVF, HNSW)
- Exact cosine similarity by default
- **Checkpoint**: `checkpoints/index_building.json`

## ğŸ” Advanced Features

### Smart Resume System
```bash
# Skip completed phases automatically
python run_training.py shirts  # Resumes from last checkpoint

# Force complete rebuild
python run_training.py shirts --force
```

### Performance Configuration
Modify `core/config.py`:
```python
# Download settings
max_images_per_product = 10      # Images per product
download_timeout = 10            # Download timeout (seconds)
max_workers = 8                  # Concurrent downloads

# Model settings
device = "auto"                  # auto/cuda/cpu
batch_size = 16                  # CLIP batch size
embedding_dim = 512              # Feature dimensions

# Index settings
index_type = "flat"              # flat/ivf/hnsw
similarity_metric = "cosine"     # cosine/euclidean
```

### Custom Index Types
```python
# For large datasets (>100k products)
config.index.index_type = "ivf"

# For maximum speed (approximate search)
config.index.index_type = "hnsw"
```

## ğŸ§ª Testing & Validation

```bash
# System validation
python test_setup.py

# Health check
curl http://localhost:8000/health

# Model information
curl http://localhost:8000/models

# Test search (with image file)
curl -X POST \
  -F "file=@test_image.jpg" \
  "http://localhost:8000/general/search?top_k=5"
```

## ğŸ“ˆ Monitoring & Logging

### Log Files
- **Pipeline logs**: `logs/pipeline.log`
- **API logs**: `logs/api.log`
- **Training logs**: `logs/training.log`

### Progress Tracking
- Real-time progress bars with ETA
- Detailed checkpoint information
- Performance metrics and timing

### Health Monitoring
```python
# API health endpoint returns:
{
    "status": "healthy",
    "available_models": 2,
    "total_products": 3500,
    "models": {
        "general": {"products": 175, "name": "General Product Search"},
        "shirts": {"products": 3325, "name": "Specialized Shirts Search"}
    }
}
```

## ğŸ”§ Troubleshooting

### Common Issues

**Model Not Training**
```bash
# Check CSV file location
ls data/your_model/products.csv

# Check logs
tail -f logs/training.log

# Force rebuild
python run_training.py your_model --force
```

**API Connection Issues**
```bash
# Check if API is running
curl http://localhost:8000/health

# Restart API
python run_api.py
```

**Memory Issues**
```python
# Reduce batch size in core/config.py
config.model.batch_size = 8

# Use CPU-only mode
config.model.device = "cpu"
```

### Performance Optimization

**For Large Datasets (>10k products)**:
- Use `index_type = "ivf"` for faster approximate search
- Increase `batch_size` if you have GPU memory
- Consider distributed processing for huge datasets

**For Production Deployment**:
- Use GPU acceleration for faster feature extraction
- Enable CORS properly for web deployment
- Set up proper logging and monitoring
- Use load balancing for high-traffic scenarios

## ğŸ“„ License

[Your License Here]

## ğŸ¤ Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ†˜ Support

For issues, questions, or feature requests:
- ğŸ“§ Email: [your-email]
- ğŸ› Issues: [GitHub Issues URL]
- ğŸ“– Docs: [Documentation URL]

---

**Built with â¤ï¸ using OpenAI CLIP, FAISS, FastAPI, and Streamlit** 